{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Bagging**\n",
    "\n",
    "Bagging, short for Bootstrap Aggregating, is an ensemble machine learning technique that improves model performance by reducing variance. The key idea is to create multiple versions of the training dataset by resampling with replacement (bootstrap sampling), train a model on each version, and then combine the predictions.\n",
    "\n",
    "steps:\n",
    "\n",
    "1.Bootstrap Sampling: Randomly sample the training dataset multiple times with replacement to create several smaller datasets (each the same size as the original).\n",
    "\n",
    "2.Model Training: Train a model (e.g., decision trees) on each of these datasets/samples.\n",
    "\n",
    "3.Aggregation****: For regression, average the predictions; for classification, use majority voting across the models.\n",
    "\n",
    "\n",
    "Note**: Bagging is commonly used in algorithms like Random Forest, where multiple decision trees are trained and their results combined for better performance."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#Bootstrap sampling or Resampling with replacement: Where subset can have same samples multile times.\n",
    "\n",
    "![Getting Started](1.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Randomforest is one of the bagging technique with one distinction...here we not only sample data rows...but also sample features(columns).\n",
    "\n",
    "Bagging vs Bagged Tress:\n",
    "In bagging underlying model can be anything(SVM, Knn, Logistic regression etc.)\n",
    "In Bagged tress....each model is a tree."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Boosting**\n",
    "\n",
    "Boosting is an ensemble machine learning technique that focuses on improving the accuracy of weak learners (models that perform slightly better than random guessing). Unlike bagging, where multiple models are trained independently, boosting trains models sequentially, with each model attempting to correct the errors of its predecessor.\n",
    "\n",
    "Key Concepts of Boosting:\n",
    "\n",
    "Sequential Learning: Models are trained one after another, and each subsequent model attempts to correct the mistakes of the previous ones.\n",
    "\n",
    "Weighted Errors: In boosting, misclassified data points are given more weight so that the next model focuses on them.\n",
    "\n",
    "Combining Models: The predictions of all models are combined (often by weighted voting or averaging) to make the final prediction.\n",
    "\n",
    "\n",
    "AdaBoost (Adaptive Boosting)\n",
    "AdaBoost is one of the earliest and most popular boosting algorithms. It works by training multiple weak learners (usually decision trees with one level, called \"stumps\") and combines them to form a strong learner. The idea is to focus on the examples that previous classifiers misclassified.\n",
    "\n",
    "How AdaBoost Works:\n",
    "Initialization: Start by assigning equal weights to all the training data points.\n",
    "\n",
    "Training:\n",
    "\n",
    "Train the first weak learner (e.g., a decision stump).\n",
    "\n",
    "Calculate the error of this weak learner on the training data.\n",
    "\n",
    "Increase the weights of the misclassified data points so that the next weak learner focuses on them more.\n",
    "\n",
    "Repeat: Train subsequent weak learners on the updated weighted data, with each learner correcting the mistakes of the previous one.\n",
    "\n",
    "Final Prediction: Combine all the weak learners' predictions using a weighted majority vote (in classification) or weighted average (in regression).\n",
    "\n",
    "\n",
    "Differences Between Boosting and Bagging:\n",
    "Boosting focuses on reducing bias by learning from the mistakes of previous models, while bagging (like Random Forest) focuses on reducing variance by training models independently.\n",
    "\n",
    "In boosting, models are built sequentially, whereas in bagging, models are built in parallel."
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
