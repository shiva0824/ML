{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The XGBRegressor is an implementation of gradient boosting for regression problems using the XGBoost library, which is known for its performance and speed. XGBoost stands for Extreme Gradient Boosting and is a popular machine learning algorithm, particularly for structured/tabular data.\n",
    "\n",
    "Key Features of XGBRegressor:\n",
    "\n",
    "Boosting: It builds an ensemble of weak learners (typically decision trees) sequentially, where each new model corrects the errors of the previous ones.\n",
    "\n",
    "Regularization: It includes L1 (Lasso) and L2 (Ridge) regularization to prevent overfitting.\n",
    "\n",
    "Handling Missing Values: XGBoost automatically handles missing data, so you donâ€™t need to worry about imputing them before fitting.\n",
    "\n",
    "Parallelization: XGBoost uses parallel and distributed computing, making it faster compared to traditional gradient boosting implementations."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "xgb_reg = xgb.XGBRegressor(\n",
    "    n_estimators=100,        # Number of trees\n",
    "    learning_rate=0.1,       # Step size shrinkage\n",
    "    max_depth=3,             # Maximum depth of a tree\n",
    "    subsample=0.8,           # Subsample ratio of the training instances\n",
    "    colsample_bytree=0.8,    # Subsample ratio of columns for each split\n",
    "    objective='reg:squarederror'  # Loss function\n",
    ")\n",
    "\n",
    "Key Hyperparameters to Tune:\n",
    "\n",
    "n_estimators: Number of trees (boosting rounds).\n",
    "\n",
    "learning_rate: Step size shrinkage used to prevent overfitting. A smaller value requires more trees (n_estimators).\n",
    "\n",
    "max_depth: Maximum depth of each tree, controlling model complexity.\n",
    "\n",
    "subsample: Fraction of the training data to be used in each round, used to prevent overfitting.\n",
    "\n",
    "colsample_bytree: Fraction of features to be randomly sampled for each tree.\n",
    "\n",
    "objective: Defines the loss function (for regression, typically reg:squarederror or reg:squaredlogerror)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Advanced Techniques:\n",
    "\n",
    "Cross-validation: Use xgb.cv to find the best number of boosting rounds with early stopping.\n",
    "\n",
    "Grid Search/Randomized Search: Combine with GridSearchCV or RandomizedSearchCV from sklearn to find optimal hyperparameters.\n",
    "\n",
    "Feature Importance: XGBoost provides built-in functionality to retrieve feature importance."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Tuning Tips:\n",
    "\n",
    "If your model is overfitting, decrease max_depth, reduce learning_rate, or use subsample and colsample_bytree.\n",
    "\n",
    "If your model is underfitting, increase n_estimators, learning_rate, or max_depth."
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
