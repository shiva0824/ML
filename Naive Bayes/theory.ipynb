{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Bayes theorem:\n",
    "P(A/B) = P(B/A)P(A)/P(B)\n",
    "\n",
    "The reason it's called naive bayes...is we make a 'naive' assumption that features are independent of each other.\n",
    "\n",
    "Naive bayes classifier:\n",
    "It is a combination of bayes theorem and naive assumption that 2 events are going to be independent when they may not be."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![Getting Started](images/1.png)\n",
    "![Getting Started](images/2.png)\n",
    "![Getting Started](images/3.png)\n",
    "![Getting Started](images/4.png)\n",
    "![Getting Started](images/5.png)\n",
    "![Getting Started](images/6.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "GaussianNB is a classification algorithm from the Naive Bayes family, implemented in the sklearn (scikit-learn) library in Python. It's based on Bayes' Theorem and assumes that the features follow a Gaussian (normal) distribution. This is particularly useful for continuous data.\n",
    "\n",
    "Key Points about GaussianNB:\n",
    "\n",
    "Assumption: The algorithm assumes that each feature is distributed according to a Gaussian distribution.\n",
    "\n",
    "Performance: Naive Bayes can perform surprisingly well with less data, but the Gaussian assumption may not hold for all datasets.\n",
    "\n",
    "Speed: Naive Bayes is fast and works well for large datasets.\n",
    "\n",
    "Handling continuous data: GaussianNB is specifically designed for continuous data, making it useful when the features are numerical.\n",
    "\n",
    "Parameters:\n",
    "\n",
    "priors: Prior probabilities of the classes. If not provided, the class prior probabilities are learned from the data.\n",
    "\n",
    "var_smoothing: Portion of the largest variance of all features that is added to variances for stability."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "source": [
    "#CountVectorizer \n",
    "\n",
    "It is a feature extraction tool in the sklearn.feature_extraction.text module of the scikit-learn library.\n",
    "\n",
    "It is commonly used in Natural Language Processing (NLP) to convert a collection of text documents into a matrix of token counts. \n",
    "\n",
    "Each column in the resulting matrix represents a unique token (word), and each row represents a document.\n",
    "\n",
    "\n",
    "How CountVectorizer Works:\n",
    "\n",
    "Tokenization: It splits the text into individual words or tokens.\n",
    "\n",
    "Vocabulary Building: It builds a vocabulary of all tokens from the corpus.\n",
    "\n",
    "Count Matrix: It creates a matrix where each cell contains the count of occurrences of each token in each document.\n",
    "\n",
    "\n",
    "Key Methods:\n",
    "\n",
    "fit_transform(corpus): Learns the vocabulary from the input text and returns the document-term matrix.\n",
    "\n",
    "get_feature_names_out(): Returns the vocabulary (unique tokens) learned during the fit.\n",
    "\n",
    "toarray(): Converts the sparse matrix returned by fit_transform into a dense array.\n",
    "\n",
    "Parameters:\n",
    "\n",
    "max_df: Ignore terms that have a document frequency higher than the given threshold (float between 0 and 1, or integer).\n",
    "\n",
    "min_df: Ignore terms that have a document frequency lower than the given threshold.\n",
    "\n",
    "stop_words: List of stop words to remove from the vocabulary (common words like \"and\", \"the\").\n",
    "\n",
    "ngram_range: The lower and upper boundary for n-grams. For example, (1, 2) will consider both unigrams and bigrams."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the below image...get_feature_names contains the vocabulary of our corpus..i.e unique words from each document.\n",
    "\n",
    "and matrix contains ho many times that appear in each document..\n",
    "\n",
    "for example 2 in our matrix indicates...word 'document' appeared 2 times in 2nd line/document/email"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![Getting Started](images/7.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#Multinomial Naive Bayes\n",
    "\n",
    "(Multinomial NB) is a variant of the Naive Bayes classifier that is particularly suited for discrete data, such as text classification. \n",
    "\n",
    "It assumes that the features (e.g., word counts) follow a multinomial distribution. \n",
    "\n",
    "This makes it suitable for scenarios where the input features are counts or frequencies, like term frequencies in text documents.\n",
    "\n",
    "\n",
    "Features of Multinomial Naive Bayes:\n",
    "\n",
    "Assumption:\n",
    "Multinomial NB assumes that the features (e.g., words in a document) are generated from a multinomial distribution, meaning it focuses on the count of occurrences of each feature.\n",
    "\n",
    "Applications:\n",
    "Commonly used for text classification tasks such as spam detection, sentiment analysis, and topic classification.\n",
    "\n",
    "\n",
    "Key Parameters:\n",
    "\n",
    "alpha: Smoothing parameter to avoid zero probabilities. Common values are 1.0 for Laplace smoothing.\n",
    "\n",
    "fit_prior: Whether to learn class prior probabilities. If set to True, the class priors will be estimated from the training data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "diff and When to use what\n",
    "\n",
    "![Getting Started](images/7.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "source": [
    "#Sklearn pipeline.....where we can define pipeline of transformation\n",
    "\n",
    "A Pipeline in scikit-learn is a way to streamline a machine learning workflow by chaining together a sequence of data processing steps and model training. This makes it easier to manage the workflow and ensures that the same transformations are applied consistently to both training and testing data.\n",
    "\n",
    "Simplifies Code: Encapsulates multiple steps into a single object.\n",
    "\n",
    "Prevents Data Leakage: Ensures that data preprocessing steps (like scaling or encoding) are applied only to the training data during fitting and the same transformations are applied to the test data.\n",
    "\n",
    "Easier Hyperparameter Tuning: Allows for straightforward use with tools like GridSearchCV or RandomizedSearchCV.\n",
    "\n",
    "How to Create a Pipeline:\n",
    "\n",
    "Import Necessary Classes: You will typically need estimators (like classifiers or regressors) and transformers (like scalers or vectorizers).\n",
    "\n",
    "Define the Pipeline: Use make_pipeline() or Pipeline() to define your sequence of steps.\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
